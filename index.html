<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Duo Zheng</title>

  <meta name="author" content="Duo Zheng (郑铎)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Duo Zheng (郑铎)
                  </p>
                  <p>I am a second-year Ph.D. student at LaVi Lab, The Chinese University of Hong Kong. I am
                    fortunate to be advised by <a href="https://lwwangcse.github.io/">Prof. Liwei Wang</a>. Before that,
                    I received B.Eng. and M.S from Beijing University of Posts and Telecommunications (2016-2023),
                    advised by <a href="https://scholar.google.com.hk/citations?user=-RtarngAAAAJ&hl=zh-CN&oi=ao">Prof.
                      Xiaojie Wang</a>.
                  </p>
                  </p>
                  <p> I am interested in Vision-and-Language, Embodied AI and LLMs.
                  </p>
                  <p style="text-align:center">
                    <a href="dzheng23@link.cuhk.edu.hk">Email</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=HMSnVjUAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://github.com/zd11024/">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/me.jpg"><img style="width:100%;max-width:100%;object-fit: cover; "
                      alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>  
              <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/vgllm.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2505.24625">
                    <span class="papertitle">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>*, Shijia Huang*, Yanyang Li, Liwei Wang
                  <br>
                  Arxiv 2025.</strong>
                  <br>
                  <a href="https://github.com/LaVi-Lab/VG-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2505.24625">Paper</a>
                  <p>
                  We present a novel framework to enhance MLLMs’ 3D spatial understanding capability, which incorporates a 3D visual geometry encoder to provide latent 3D geometric information given only video inputs.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/video3dllm.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2412.00493">
                    <span class="papertitle">Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding</span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>*, Shijia Huang*, Liwei Wang
                  <br>
                  <em>CVPR</em>, 2025.</strong>
                  <br>
                  <a href="https://github.com/LaVi-Lab/Video-3D-LLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2412.00493">Paper</a>
                  <p>
                   This paper proposes a Video-based LLM for 3D scene understanding, which is built upon a Video LLM and incorporates 3D coordinates into video representations.
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/navillm.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2312.02010">
                    <span class="papertitle">Towards Learning a Generalist Model for Embodied Navigation</span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>*, Shijia Huang*, Lin Zhao, Yiwu Zhong, Liwei Wang
                  <br>
                  <em>CVPR</em>, 2024. <strong style="color: rgba(214, 22, 22, 0.511);">(Poster highlight, Top 2.8%)</strong>
                  <br>
                  <a href="https://github.com/zd11024/NaviLLM">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2312.02010">Paper</a>
                  <p>
                    This paper proposes the first generalist model for embodied navigation, NaviLLM. It adapts LLMs to
                    embodied navigation by introducing schema-based instruction.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/cleva.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2308.04813">
                    <span class="papertitle">CLEVA: Chinese Language Models EVAluation Platform</span>
                  </a>
                  <br>
                  Yanyang Li, Jianqiao Zhao, <strong>Duo Zheng</strong>, Zi-Yuan Hu, Zhi Chen, Xiaohui Su, Yongfeng
                  Huang, Shijia Huang, Dahua Lin, Michael R. Lyu, Liwei Wang
                  <br>
                  <em>EMNLP, 2023, System Demonstrations</em>
                  <br>
                  <a href="https://github.com/LaVi-Lab/CLEVA">Project</a>
                  /
                  <a href="https://arxiv.org/pdf/2308.04813">Paper</a>
                  <p>
                    CLEVA provides a comprehensive benchmark to holistically evaluate Chinese LLMs.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/UniRef.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2210.13076">
                    <span class="papertitle">Towards Unifying Reference Expression Generation and Comprehension </span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>, Tao Kong, Ya Jing, Jiaan Wang, Xiaojie Wang
                  <br>
                  <em>EMNLP, 2022, Long Paper</em>
                  <br>
                  <a href="https://github.com/zd11024/UniRef">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2210.13076">Paper</a>
                  <p>
                    This paper proposes a unified model for reference expression generation and comprehension.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/DialDiff.png' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548170">
                    <span class="papertitle">Visual Dialog for Spotting the Differences between Pairs of Similar Images</span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>, Fandong Meng, Qingyi Si, Hairun Fan, Zipeng Xu, Jie Zhou, Fangxiang Feng, Xiaojie Wang
                  <br>
                  <em>ACM MM, 2022</em>
                  <br>
                  <a href="https://github.com/zd11024/Spot_Difference">Code</a>
                  /
                  <a href="https://dl.acm.org/doi/abs/10.1145/3503161.3548170">Paper</a>
                  <p>
                    We propose a cooperative object-referring game Dial-the-Diff, where the goal is to locate the different object between two similar images via conversing between questioner and answerer.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/emnlp2021.jpg' width=100%>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2109.02297">
                    <span class="papertitle">Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser</span>
                  </a>
                  <br>
                  <strong>Duo Zheng</strong>, Zipeng Xu, Fandong Meng, Xiaojie Wang, Jiaan Wang, Jie Zhou
                  <br>
                  <em>Findings of EMNLP, 2021, Long Paper</em>
                  <br>
                  <a href="https://github.com/zd11024/Entity_Questioner">Code</a>
                  /
                  <a href="https://arxiv.org/abs/2109.02297">Paper</a>
                  <p>
                    In this paper, we propose Related entity enhanced Questioner (ReeQ) and Augmented Guesser (AugG) to enhance Visual Dialog Questioner in both SL and RL.                 
                  </p>
                </td>
              </tr>


            </tbody>
          </table>

          <!-- internship -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Internships</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <ul>
            <!-- <li>
              <p>Shanghai AI Laboratory</p>
            </li> -->
            <!-- <li>
              <p style="line-height: 1.5;"><b>Sensetime Research</b>, Beijing, China. July 2022 - Sept 2023.
              </br>
              Research Internship, working on Visual-Language Reasoning.
              </p>
            </li> -->
            <li>
              <p style="line-height: 1.5;" ><b>ByteDance Research</b>, Beijing, China. Jan 2022 - June 2022.
                </br>
              Research Internship, focusing on Visual Grouding.
              </br>
              Mentor: <a href="https://www.taokong.org/">Tao Kong</a>
            </p>
            </li>
            <li>
              <p style="line-height: 1.5;" ><b>WeChat AI, Tencent Inc.</b>, Beijing, China. Sept 2020 - Jan 2020.
                </br>
              Research Internship, focusing on Visual Dialog.
              </br>
              Mentor: <a href="https://fandongmeng.github.io/">Fandong Meng</a>
            </p>
            </li>
          </ul>

          <!-- honors and awards -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Selected Honors</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <ul>
            <li>
              <p>Postgraduate Scholarship, The Chinese University of Hong Kong. 2023 - Present</p>
            </li>
            <li>
              <p>China National Scholarship. Ministry of Education of P.R. China. 2022.</p>
            </li>
            <li>
              <p>CCF Elite Collegiate Student Award. China Computer Federation. 2020.</p>
            </li>
            <li>
              <p>Gold medal, The 5th China Collegiate Programming Contest (Qinhuangdao Site). 2019. </p>
            </li>
            <li>
              <p>The First‑grade Award, Chinese High School Mathematics League. 2015. </p>
            </li>
          </ul>

          <br>
          <hr>

          <!-- service -->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <h2>Service</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <ul>
            <li>
              <p>Reviewer: CVPR, ICCV, ECCV, ICML, NeurIPS, ACL, ACM MM, IJCV</p>
            </li>
            <li>
              <p>Teaching: AIST 1000, CSCI 3320, AIST 3120</p>
            </li>
          </ul>

          <br>
          <hr>

        </td>
      </tr>
    </tbody>
  </table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:center;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
              website</a>
          </p>
        </td>
      </tr>
    </tbody>
  </table>
  </td>
  </tr>
  </table>
</body>

</html>